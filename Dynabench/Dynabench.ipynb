{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d35e61",
   "metadata": {},
   "source": [
    "# Dynatask - The new pattern of setting the benchmarking in AI community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c3fa8",
   "metadata": {},
   "source": [
    "![download](https://user-images.githubusercontent.com/15075906/135279813-d930e1cc-9b7b-4a49-a178-d615cff6ac5b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e83eb3",
   "metadata": {},
   "source": [
    "Dynabench is a research tool for collecting and benchmarking dynamic data. Static benchmarks contain well-documented flaws."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d99cd7a",
   "metadata": {},
   "source": [
    "There was a time when Facebook first introduced this feature a year ago. This is the first generation of a platform that rethinks the field of AI. We can now construct our own custom tasks to better evaluate the performance of natural language processing with the help of this... For free, it offers more flexibility, dynamic, and realistic settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea62adb",
   "metadata": {},
   "source": [
    "In essence, the dynatask now has a new feature.... It's simple for typical human annotators to deceive NLp models. Identify flaws through observing natural interactions. These methods indicate how individuals act and react in comparison to the preceding standardization's benchmarks. The primary problem is that it focuses on fixed data points and is susceptible to saturation. We can also compare models on our dynamic leaderboard using evaluation-as-a-service capabilities, which goes beyond accuracy and investigates a more holistic measurement of fairness, resilience, computation, and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ea60f",
   "metadata": {},
   "source": [
    "Dynabench is a benchmarking platform that addresses a variety of well-known AI concerns, such as model evaluation. The dynabench's main goal is to solve the difficulty highlighted by: Saturation a) Bias b) Alignment (c) d) The culture of the leaderboard f) Repeatability g) Backward compatibility f) Accessibility h) Usefulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea58809",
   "metadata": {},
   "source": [
    "Dynabench was initially launched with four tasks:\n",
    "\n",
    "1)Yixin Nie and Mohit Bansal of UNC Chapel Hill invented natural language inference.\n",
    "\n",
    "\n",
    "2)UCL's Max Bortolo, Pontus Stenetorp, and Sebastian Riedel created the question-answering game....\n",
    "\n",
    "3)Stanford's Atticus Geiger and Chris Potts invented sentiment analysis.\n",
    "\n",
    "4)Hate speech identification was developed by Turing Institute's Bertie Vidgen and University of Sheffield/Simon Fraser University's Zeerak Waseem Talat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18460428",
   "metadata": {},
   "source": [
    "Facebook has essentially launched the following in the last year:\n",
    "\n",
    "1)Task: Answering visual questions\n",
    "\n",
    "2)Machine translation jobs with limited resources\n",
    "\n",
    "3)At the Workshop for Machine Translations, powered the multilingual translation challenge..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013e278",
   "metadata": {},
   "source": [
    "The dynamic data is being gathered; thus far, eight articles have been published, 400k raw instances have been classified, and four open source large scale data sets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc0832",
   "metadata": {},
   "source": [
    "These dynabench features provide up a lot of possibilities... a)Just like it only requires a little coding skill. b)Annotation interfaces can be easily customised. c)Allow interactions with Dynabench-hosted models. d)improves the accessibility of dynamic adversarial data gathering to the research community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de47096",
   "metadata": {},
   "source": [
    "The long-term success of AI depends on high-quality and holistic model evaluation, and we believe that Dynabench, as a collaborative effort, will play a significant part in the future of benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef4eb6",
   "metadata": {},
   "source": [
    "# The Overall Use of Dynabench:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae89854",
   "metadata": {},
   "source": [
    "Dynatask is extremely adaptable and adaptable. A single task can have one or more owners, each of whom defines the work's settings. a) In the evaluation-as-a-service framework, for example, users can choose which existing data sets they want to employ. b) They can quantify model performance using a range of evaluation measures, including not only accuracy but also robustness, fairness, computation, and memory. c)Anyone can upload models to a task's evaluation cloud, which computes scores and other metrics based on the data sets selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa5fe9",
   "metadata": {},
   "source": [
    "d)After those models have been uploaded and evaluated, they can be looped in for dynamic data gathering and human-in-the-loop evaluation. e)Task users can also collect data using the dynabench.org web interface or annotators (such as Mechanical Turk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011523e4",
   "metadata": {},
   "source": [
    "The steps involved in benchmarking Natural Language Inference jobs are as follows:\n",
    "\n",
    "Step 1: Log into your Dynabench account and go to your profile page to fill out the \"Request new job\" form.\n",
    "\n",
    "Step 2:As the task owner, you will have access to a specific task page and associated admin dashboard once it has been accepted.\n",
    "\n",
    "Step 3: On the dashboard, select the existing datasets you want to use to evaluate models when they're uploaded, as well as the metrics you want to utilise.\n",
    "\n",
    "Step 4: Next, propose baseline models or request that the community do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca94d0",
   "metadata": {},
   "source": [
    "Step 5: You can then upload fresh contexts to the system and begin collecting data through the task owner interface if you wish to gather a new round of dynamic adversarial data, in which annotators are required to design instances that deceive the model.\n",
    "\n",
    "\n",
    "Step 6: Once you have enough data and find that training on it helps the system improve, you may upload better models and then put them in the data collection loop to develop even more powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c460e",
   "metadata": {},
   "source": [
    "# Started with Dynatask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82d0bb",
   "metadata": {},
   "source": [
    "1)The community is at the heart of Dynabench. To enable the AI community to investigate more comprehensive, holistic, and repeatable approaches to model evaluation. \n",
    "\n",
    "2) The main goal is to make it simple for anyone to create high-quality human-in-the-loop data sets. \n",
    "\n",
    "3)With Dynatask, you can go beyond accuracy-only leaderboards and evaluate AI models holistically. \n",
    "\n",
    "4)Now is the time to create new examples that deceive existing models, upload new models for evaluation, or submit your own Dynabench job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083711a",
   "metadata": {},
   "source": [
    "Dynabench is an open-source framework for creating dynamic datasets and evaluating models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dba8b6",
   "metadata": {},
   "source": [
    "It operates in a web browser and allows you to create datasets with both a human and a model in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada3743",
   "metadata": {},
   "source": [
    "Dynabench meets the following needs in our community: Contemporary models excel on benchmark tasks but fall short on simple challenge cases and in real-world circumstances..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da5d1f",
   "metadata": {},
   "source": [
    "Dataset design, model development, and model evaluation may all be directly informed by Dynabench, resulting in more robust and informative benchmarks..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa51e50",
   "metadata": {},
   "source": [
    "Machine learning models used to take decades to outperform human estimates on benchmark tasks, but that is no longer the case.\n",
    "\n",
    "NLP, like the rest of AI, has progressed quickly as a result of advances in technology.\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ::::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee181b0",
   "metadata": {},
   "source": [
    "The introduction of standards that facilitate the development of ambitious new data-driven models and encourage apples-to-apples model comparisons has been equally crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2d399",
   "metadata": {},
   "source": [
    "Given these improvements, it's easy to believe that NLP has produced models with human-like linguistic capabilities. Users are aware that, despite our efforts, we are still a long way from achieving this aim.\n",
    "\n",
    "We feel the moment has come to reconsider benchmarking completely. In Dynabench, we must take a stand and attempt to provide a partial solution.\n",
    "\n",
    "Dynabench, an open-source, web-based research tool for dynamic data collecting and model benchmarking, was created in response to this critical role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537466c",
   "metadata": {},
   "source": [
    "Dynabench contains tasks in which we collect data in the loop and compare it to state-of-the-art models. When engaging with humans, the stronger the models are and the fewer flaws they have, the lower their error rate will be, providing us with a tangible metric. —\n",
    "\n",
    "In other words, how successfully do AI systems interact with humans? This exposes the flaws in current models while also providing useful training and assessment data for the community to use in developing even better models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74898a",
   "metadata": {},
   "source": [
    "Dynabench is a platform that may be used for a variety of tasks.\n",
    "\n",
    "\n",
    "Each task's data is gathered in numerous rounds, each beginning with the current state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd52898",
   "metadata": {},
   "source": [
    "One or more target models are \"in the loop\" in every round. Humans, whether professional linguists or crowdworkers, interact with these models and can spot flaws in them by supplying instances for an optional context. Other humans can verify the validity of examples that models get wrong or struggle with. The information gathered in this process can be utilised to evaluate state-of-the-art models and to train even more powerful ones, hopefully resulting in a virtuous cycle that aids in the advancement of the discipline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4886040",
   "metadata": {},
   "source": [
    "<<<<<>>>>>>>>>>>>>>>>>>>\n",
    "The platform is intended to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community as a large-scale collaborative endeavour.\n",
    "The infrastructure is now built up for dynamic adversarial data collection, where humans can search for modelfooling examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3da81",
   "metadata": {},
   "source": [
    "Because the average case, as measured by maximum likelihood training on i.i.d. datasets, is much less interesting than the worst (i.e., adversarial) case, which is what we want our systems to be able to handle if they are put in critical systems that interact with humans in real-world settings, this design choice was made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abbbd4",
   "metadata": {},
   "source": [
    "However, Dynabench is not restricted to adversarial settings, and one might conceive scenarios in which people are rewarded for uncovering examples that models, even if they are correct, are highly uncertain about, possibly in an active learning context, rather than for deceiving a model or ensemble of models.\n",
    "Similarly, the paradigm works well in collaborative contexts that rely on human feedback or even bargaining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7c128",
   "metadata": {},
   "source": [
    "The fact that models and humans interact live \"in the loop\" for evaluation and data collecting is a critical component of this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f04f4",
   "metadata": {},
   "source": [
    "# Architecture of Dynabench:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5812bb",
   "metadata": {},
   "source": [
    "![242147582_401095008070718_5323071582819492841_n](https://user-images.githubusercontent.com/15075906/135280372-655c700f-e1d1-4794-8415-e266ec78b87d.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c491b",
   "metadata": {},
   "source": [
    "# Features and Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72a0a4",
   "metadata": {},
   "source": [
    "Dynabench provides real-time, low-latency feedback on the behaviour of cutting-edge NLP models. The technological stack is built on PyTorch (Paszkeet al., 2019), with TorchServe serving the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027cab2",
   "metadata": {},
   "source": [
    "The platform not only displays prediction probabilities, but also allows users to view token-level layer integrated gradients (Sundararajan et al., 2017) computed using the Captum interpretability library via a \"inspect model\" option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0d214",
   "metadata": {},
   "source": [
    "We let the user describe what the proper label is for each case, as well as why they think it tricked a model if the model got it wrong, or why the model might have been fooled if it wasn't. To confirm their validity, all gathered model-fooling (or, depending on the task, even non-model-fooling) cases are validated by other individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc7cef",
   "metadata": {},
   "source": [
    "Task owners can gather examples by using the web interface, interacting with the community, or using Mephisto3, which makes it simple to link, for example, Mechanical Turk workers to the same backend. All of the information gathered will be open sourced and anonymised..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2105a",
   "metadata": {},
   "source": [
    "# Initial task featured by Dynabench:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79c2eb",
   "metadata": {},
   "source": [
    "<img width=\"1920\" alt=\"242800113_1144504696075679_2645522933893632776_n\" src=\"https://user-images.githubusercontent.com/15075906/136352610-eb49aeed-7429-42a9-b891-efa7963cfd75.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6528a",
   "metadata": {},
   "source": [
    "Inference from natural language. Built on the semantic foundations of natural logic (Sánchez Valencia, 1991, for example) and tracing its roots much further back (vanBenthem, 2008),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467e851",
   "metadata": {},
   "source": [
    "NLI is one of the quintessential natural language understanding tasks. NLI, also known as ‘recognizing textual entailment’ (Dagan et al., 2006), is often formulated as a 3-way classification problem where the input is a context sentence paired with a hypothesis, and the output is a label (entailment, contradiction, or neutral) indicating the relation between the pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf11b9",
   "metadata": {},
   "source": [
    "To seed the Dynabench NLI task, we use the ANLI dataset (Nie et al., 2020) and its three rounds.\n",
    "The annotators were given a context (extracted from a pre-selected corpus) and a desired target label during the ANLI data collecting phase, and asked to suggest a hypothesis that would deceive the target model adversary into misclassifying the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b41c1",
   "metadata": {},
   "source": [
    "If the target model is deceived, the annotator is asked to conjecture or justify why their example is correct. Dynabench introduces various advances, including the use of an ensemble of two separate models with different architectures as target adversaries to increase diversity and resilience, as well as the selection of candidate contexts from a more diverse set of Wikipedia featured articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a61c68",
   "metadata": {},
   "source": [
    "Furthermore, the adversary ensemble will aid in avoiding problems associated with constructing a dataset whose distribution is too closely linked to a specific target model or architecture. We're also looking for two forms of natural language explanations: why an example is correct and why a target model can be incorrect. We anticipate that deciphering this data will provide another layer of interpretability to the models, making them at least as understandable as they are robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc095de",
   "metadata": {},
   "source": [
    "## Question answering. The QA task takes the same format as SQuAD1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da13825",
   "metadata": {},
   "source": [
    "![1-Figure1-1](https://user-images.githubusercontent.com/15075906/136353003-3b892d35-ce13-4b25-8a88-8d80cb062921.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202498a",
   "metadata": {},
   "source": [
    "1) Given a context and a question, extract an answer as a continuous span of text from the context. \"Beat the AI\" provided the first batch of adversarial QA (AQA) data (Bartolo et al., 2020).\n",
    "\n",
    "2) During annotation, crowd workers were given a Wikipedia context that was identical to those in SQuAD1.1, and they were asked to create a query and choose a response.\n",
    "\n",
    "3) A wordoverlap F1 threshold was used to compare the annotated answer to the model prediction, and if the two were significantly dissimilar, the model was determined to have been tricked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0873c",
   "metadata": {},
   "source": [
    "4)In round one, the target models were BiDAF (Seo et al., 2017), BERT-Large, and RoBERTa-Large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40b80e",
   "metadata": {},
   "source": [
    "The current round's model is RoBERTa mixed with SQuAD1.1, which was trained on the first round's examples.\n",
    "\n",
    "Despite the superhuman performance on SQuAD1.1, machine performance on the current leaderboard is still far below that of humans.\n",
    "\n",
    "In this phase, we want to collect a large number of different examples, with a focus on strengthening model robustness by generative data augmentation, in order to create more challenging model opponents in this confined task environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b445e9a4",
   "metadata": {},
   "source": [
    "We should stress that we do not believe this job structure to be typical of the broader concept of closed-domain QA, and we are working to broaden it to include unanswerable questions, longer and more complicated passages, and so on. Questions with a yes/no answer\n",
    "\n",
    "<<<>>>\n",
    "\n",
    "The sentiment analysis project is a multi-faceted effort to develop a dynamic benchmark for sentiment analysis and to test some of Dynabench's basic concepts. An early report and the first two rounds of this dataset are provided by Potts et al. (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602b97b",
   "metadata": {},
   "source": [
    "The task is broken down into three categories: positive, negative, and neutral.\n",
    "\n",
    "The purpose of utilising a simple positive/negative dichotomy is to demonstrate that there are still some really difficult phenomena in this traditional sentiment space.\n",
    "\n",
    "The neutral category was added to avoid (and to assist trained models in avoiding) the mistaken assumption that every text includes sentiment information (Pang and Lee, 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1959cca",
   "metadata": {},
   "source": [
    "1)In the first step, we investigated how to best elicit diverse, creative, and naturalistic examples from employees.\n",
    "\n",
    "2)In the \"prompt\" scenario, workers are given an actual line from an existing product or service review and asked to change it in such a way that the model is fooled. Workers in the \"no prompt\" condition strive to come up with original sentences to deceive the model.\n",
    "\n",
    "3)We discover that the “prompt” condition is superior: employees are more likely to make significant adjustments, and the resulting sentences are more linguistically diverse than those in the “no prompt” condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3ea31",
   "metadata": {},
   "source": [
    "4)At the same time, we collected and validated hard sentiment instances from existing corpora, allowing us to conduct further comparisons that will aid in the refinement of the Dynabench protocols and interfaces.\n",
    "\n",
    "5)We intend to expand the collection in the future, most likely by combining attested examples with those generated using Dynabench prompts. We can address a wide range of questions about dataset artefacts, domain transfer, and overall robustness of sentiment analysis systems with these several rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbf234",
   "metadata": {},
   "source": [
    "Speech recognition is something I despise. The hate speech task determines whether or not a remark displays animosity toward a protected characteristic. Given the importance of context and speaker, as well as the range of ways in which hate can be conveyed, detecting hate is notoriously difficult (Waseem et al., 2017). For training hate detection algorithms, there are few high-quality, diverse, and large training datasets available (Vidgen and Derczynski, 2020; Poletto et al., 2020; Vidgen et al., 2019). Let organised four data collecting and model training phases, with preliminary findings published in Vidgen et al (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11dc4cd",
   "metadata": {},
   "source": [
    "Annotators are tasked with entering stuff that causes the model to make an inaccurate categorization in each round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25fdb7",
   "metadata": {},
   "source": [
    "The content is synthetic in nature because it is developed by the annotators. The model is retrained at the conclusion of each round, and the procedure is repeated. We trained a RoBERTa model on 470,000 nasty and abusive statements4 in the first round."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247a7e1",
   "metadata": {},
   "source": [
    "Due to the small pool of annotators, this aids in the identification of decision boundaries inside the model and reduces the possibility of overfitting. Content grows more adversarial with time (as seen by the fact that target models perform worse on later rounds data) and models improve (as evidenced by the fact that the model error rate decreases and the later rounds' models have the highest accuracy on each round). We use the HATECHECK diagnostic test suite to independently validate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c17ea6",
   "metadata": {},
   "source": [
    "# Caveats and Objections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b83c5",
   "metadata": {},
   "source": [
    "![Dynabench-interface](https://user-images.githubusercontent.com/15075906/136352800-d4e2fdb9-d657-4c44-9a4e-a774dc583f55.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c79ce5",
   "metadata": {},
   "source": [
    "One can offer a number of clear and valid concerns. We don't have all the solutions, but we can address some of the most common problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14008c",
   "metadata": {},
   "source": [
    "  Q.1) What if models are \"overfit\" by annotators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc123d25",
   "metadata": {},
   "source": [
    "1)A potential risk is cyclical \"progress,\" in which improved models overlook items that were important in previous rounds because annotators are too focused on a single flaw. \n",
    " \n",
    "2)Continuous learning is an attractive study direction in this area: we should aim to better understand distributional shift, as well as how to characterise how data shifts over time could impact learning and how to mitigate any negative impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11a947",
   "metadata": {},
   "source": [
    "3)It's natural to assume that the last round is the best evaluation round because of how most of us were trained, but that doesn't mean it should be the only round: in fact, the best way to evaluate progress is to evaluate on all rounds as well as any high-quality static test set that exists, possibly with a recency-based discount factor.\n",
    "\n",
    "4)Using software testing as an analogy, comparable to checklists (Ribeiro et al., 2020), it would be a terrible idea to discard old tests merely because you've developed some new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30227256",
   "metadata": {},
   "source": [
    "5) As long as prior rounds are taken into account, Dynabench's dynamic nature provides a solution to forgetting and cyclical issues: any model biases will be corrected in the end by annotators exploiting flaws. Another concern is that the data distribution may be very reliant on the loop's goal model.\n",
    "\n",
    "6)If this becomes a problem, it can be minimised by looping ensembles of many distinct architectures, such as the top current state-of-the-art ones, with numerous seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0accdf",
   "metadata": {},
   "source": [
    "Q.2) What about generative tasks? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dda64",
   "metadata": {},
   "source": [
    "For the time being, Dynabench concentrates on classification and span extraction tasks where determining whether a model is incorrect is quite simple. If the evaluation metric is ROUGE or BLEU, and we're interested in generation, we'll need a mechanism to discretize an answer to evaluate correctness because we won't have ground truth annotations, making identifying whether a model was successfully tricked more difficult. We may, however, discretize generation by rephrasing it as a multiple-choice question with hard negatives, or just asking the annotator if the generation is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42d4cb",
   "metadata": {},
   "source": [
    "To summarise, getting beyond classification will necessitate additional research, but it is certainly possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98ea9d",
   "metadata": {},
   "source": [
    "1) Dynamic benchmarking is costly, but it is worthwhile to bring the numbers together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d6d67",
   "metadata": {},
   "source": [
    "When done at the scale of our existing benchmark workloads, all data collecting efforts are expensive.\n",
    "\n",
    "2) For example, SNLI includes 20K instances that have been individually validated, and each of these examples cost about $0.50 to gather and validate (personal communication with SNLI authors). \n",
    "\n",
    "\n",
    "3) Similarly, each of MultiNLI's 40K certified instances costs $0.64. (p.c., MultiNLI authors). For ANLI cases, the average cost of creation and validation is closer to $1.00. (p.c., ANLI authors). This represents a significant rise in scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d5a9e",
   "metadata": {},
   "source": [
    "Dynamic adversarial datasets, on the other hand, may survive longer as benchmarks. If this is the case, the higher costs may end up being a good deal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdb428",
   "metadata": {},
   "source": [
    "Dynamic benchmarks, on the other hand, will tend to be more expensive than conventional benchmarks for comparable tasks, because not every annotation attempt will be model-fooling, and validation will be necessary.\n",
    ">>>>>>>>>> Such costs are almost certain to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc68521",
   "metadata": {},
   "source": [
    "when the mod els become more resilient to worker adversarial attacks, to increase in subsequent rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4651de8",
   "metadata": {},
   "source": [
    "######## The research hypothesis is that each example obtained in this manner is more valuable to the society and thus worth the investment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fdbbe",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade4d71",
   "metadata": {},
   "source": [
    "Dynabench, a research platform for dynamic benchmarking, is launched. Exploring annotator efficiency, evaluating the implications of annotator skill, and enhancing model robustness to targeted adversarial attacks in an interactive scenario are just a few of the intriguing new research topics opened up by Dynabench."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8eb9f",
   "metadata": {},
   "source": [
    "##### It also makes it easier to do additional research in dynamic data collecting and more general cross-task studies of human-machine interaction.\n",
    "The platform's current version is merely the first step on a much longer journey. They are attempting to accomplish the following objectives in the near future:\n",
    "A task may be completed by anyone. We hope to make it possible for anyone to execute their own task now that we've developed a platform that enables for human-in-the-loop model evaluation and data collecting. A target model, a (set of) context(s), and a pool of annotators are all that is required to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6a318",
   "metadata": {},
   "source": [
    ",,,,,>>>>>>>>> ..\n",
    "Multilingualism and multimodality are two terms that are often used interchangeably. Dynabench is now text-only and limited to English, but we plan to change that shortly.\n",
    "\n",
    "\n",
    "Model evaluation in real time. The evaluation of a model should not be based on a single number on a test set. When models are uploaded through a standard interface, they can be automatically assessed in a variety of ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810eb6ad",
   "metadata": {},
   "source": [
    "We'd be able to track not only accuracy, but also the use of computer resources, inference time, fairness, and a variety of other factors. As a result, dynamic leaderboards, such as those based on utility, will be possible (Ethayarajh and Jurafsky, 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8532c",
   "metadata": {},
   "source": [
    "This would also allow for backward-compatible comparisons, with no need to worry about the benchmark changing, as well as automatic inclusion of new state-of-the-art models, addressing some of the major concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab3cec",
   "metadata": {},
   "source": [
    "to fulfill reproducibility requirements, authors do not only link to their open source codebase but also to their model inference point so others can “talk with” their model. \n",
    "\n",
    "2)This will help drive progress, as it will allow others to examine models’ capabilities and identify failures to address with newer even better models.\n",
    "\n",
    "3)If we cannot always democratize the training of state-of-the-art AI models, at the very least we can democratize their evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81058ef9",
   "metadata": {},
   "source": [
    "![ineuron-logo](https://user-images.githubusercontent.com/15075906/136354734-aac78884-4d69-4581-9674-3cda5ee8d247.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff56cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
